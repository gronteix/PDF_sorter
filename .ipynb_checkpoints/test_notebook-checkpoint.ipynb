{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gustaveronteix/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import open_PDF\n",
    "import os\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas\n",
    "from shutil import copyfile\n",
    "import networkx as nx\n",
    "import shutil\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import community.community_louvain as community_louvain\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gustaveronteix/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import open_PDF\n",
    "import simple_sort\n",
    "import recursive_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = r'test_data'\n",
    "\n",
    "fname = glob.glob(os.path.join(PDF_DIR, '*.pdf'))[0]\n",
    "\n",
    "text = open_PDF.open_PDF_tika(fname)\n",
    "\n",
    "utils.get_topics_paper(text, \n",
    "                 num_words = 5, \n",
    "                 num_topics = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the different topics per paper. We generate a dict where we store:\n",
    "\n",
    " - paper name\n",
    " - position\n",
    " - list of topics\n",
    " \n",
    "We then generate a networkx graph object. The papers that have similar topics as attributes are linked together and the edges are reinforced with each new common keyword.\n",
    "\n",
    "Once the network is generated, we run Louvain community building algorithm to sort the different papers based on their common topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = r'test_data_bis'\n",
    "\n",
    "paper_dictionnary = utils.make_dict_from_papers(PDF_DIR, \n",
    "                 num_words = 5, \n",
    "                 num_topics = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle\n",
    "\n",
    "#with open('paper_dictionnary.p', 'wb') as fp:\n",
    "#    pickle.dump(paper_dictionnary, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = utils.make_graph_from_dict(paper_dictionnary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(G)\n",
    "edgewidth = [G.get_edge_data(u, v)['weight'] for u, v in G.edges()]\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, \n",
    "                       alpha=0.3, \n",
    "                       width=edgewidth, \n",
    "                       edge_color=\"m\")\n",
    "nx.draw_networkx_nodes(G, pos, \n",
    "                       node_color=\"#210070\", \n",
    "                       alpha=0.9)\n",
    "\n",
    "#label_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\": 0.7}e\n",
    "#nx.draw_networkx_labels(G, pos,\n",
    "#                        font_size=9, \n",
    "#                        bbox=label_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=40, \n",
    "                        cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words by percentage of nodes that express this word! If more than $x\\%$ of the nodes express it then remove the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = make_word_list(paper_dictionnary)\n",
    "no_keywords = get_most_common_words(word_count, 20)\n",
    "no_keywords += ['http', 'ncbi', 'experi', 'pubm', 'elsevi', 'refhub', 'cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_cleaned = utils.make_graph_from_dict(paper_dictionnary, no_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(G_cleaned)\n",
    "edgewidth = [G_cleaned.get_edge_data(u, v)['weight'] for u, v in G_cleaned.edges()]\n",
    "\n",
    "nx.draw_networkx_edges(G_cleaned, pos, \n",
    "                       alpha=0.3, \n",
    "                       width=edgewidth, \n",
    "                       edge_color=\"m\")\n",
    "nx.draw_networkx_nodes(G_cleaned, pos, \n",
    "                       node_color=\"#210070\", \n",
    "                       alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(G_cleaned, \n",
    "                                             resolution = 1.1)\n",
    "\n",
    "pos = nx.kamada_kawai_layout(G_cleaned)\n",
    "\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap('Set2', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(G_cleaned, \n",
    "                       pos, \n",
    "                       partition.keys(), \n",
    "                       node_size=40, \n",
    "                       cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G_cleaned, \n",
    "                       pos, \n",
    "                       alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(G_cleaned, \n",
    "                                             resolution = 1.1)\n",
    "\n",
    "for node in G_cleaned.nodes():\n",
    "    \n",
    "    G_cleaned.nodes[node]['partition'] = partition[node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the partition of the nodes. What we now want to do is:\n",
    " - associate a probability for each word to be in each community\n",
    " - find the relatively most probable words between the communities\n",
    " - associate each community with these most probable words\n",
    " \n",
    "Then within each community, build the subgraph with the most probable words removed and repeat the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test from dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SORTED_DIR = r'sorted_test_data_dict'\n",
    "PDF_DIR = r'test_data'\n",
    "DICTNAME = 'paper_dictionnary.p'\n",
    "n_largest_names = 2\n",
    "partition_resolution = 0.9\n",
    "n_largest_description = 9\n",
    "max_common_words = 2\n",
    "\n",
    "no_keywords = ['http', \n",
    "               'ncbi', \n",
    "               'experi', \n",
    "               'biorxiv', \n",
    "               'pubm', \n",
    "               'elsevi', \n",
    "               'refhub',\n",
    "               'dataset',\n",
    "               'licens',\n",
    "               'grant',\n",
    "               'holder',\n",
    "               'preprint',\n",
    "               'copyright',\n",
    "               'dataset',\n",
    "               'funder',\n",
    "               'intern',\n",
    "               'ncem',\n",
    "               'requir',\n",
    "               'creativecommon',\n",
    "               'certifi']\n",
    "\n",
    "simple_sort.sort_papers_from_dict(DICTNAME,\n",
    "                      PDF_DIR,\n",
    "                      SORTED_DIR,\n",
    "                      max_common_words,\n",
    "                      n_largest_names,\n",
    "                      partition_resolution,\n",
    "                      n_largest_description,\n",
    "                      no_keywords = no_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:15<00:15, 15.74s/it]"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r'test_data_bis'\n",
    "SORTED_DIR = r'sorted_data_bis'\n",
    "DICTDIR = r'dict_data_bis'\n",
    "\n",
    "n_largest_names = 2\n",
    "partition_resolution = 0.9\n",
    "n_largest_description = 9\n",
    "\n",
    "simple_sort.sort_papers_based_on_contents(PDF_DIR,\n",
    "                              SORTED_DIR,\n",
    "                              max_common_words = 2,\n",
    "                              num_words = 4,\n",
    "                              num_topics = 4,\n",
    "                              n_largest_names = 2,\n",
    "                              partition_resolution = 0.9,\n",
    "                              n_largest_description = 10,\n",
    "                              SAVEDICT = True,\n",
    "                              DICTDIR = DICTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SORTED_DIR = r'recusrive_sort_test_data_dict'\n",
    "PDF_DIR = r'test_data'\n",
    "DICTNAME = 'paper_dictionnary.p'\n",
    "n_largest_names = 2\n",
    "partition_resolution = 0.85\n",
    "n_largest_description = 9\n",
    "max_common_words = 2\n",
    "min_graph_size = 25\n",
    "\n",
    "iteration = 0\n",
    "max_depth = 4\n",
    "\n",
    "no_keywords = ['http', \n",
    "               'ncbi', \n",
    "               'experi', \n",
    "               'biorxiv', \n",
    "               'pubm', \n",
    "               'elsevi', \n",
    "               'refhub',\n",
    "               'dataset',\n",
    "               'licens',\n",
    "               'grant',\n",
    "               'holder',\n",
    "               'preprint',\n",
    "               'copyright',\n",
    "               'dataset',\n",
    "               'funder',\n",
    "               'intern',\n",
    "               'ncem',\n",
    "               'requir',\n",
    "               'creativecommon',\n",
    "               'certifi',\n",
    "               'version',\n",
    "               'fund',\n",
    "               'research']\n",
    "\n",
    "recursive_sort.recursive_sort_from_dict(DICTNAME,\n",
    "                          PDF_DIR,\n",
    "                          SORTED_DIR,\n",
    "                          max_common_words,\n",
    "                          n_largest_names,\n",
    "                          partition_resolution,\n",
    "                          n_largest_description,\n",
    "                          min_graph_size,\n",
    "                          no_keywords = no_keywords,\n",
    "                          iteration = iteration,\n",
    "                          max_depth = max_depth)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
