{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gustaveronteix/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import PyPDF2\n",
    "import glob\n",
    "import open_PDF\n",
    "import os\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas\n",
    "from shutil import copyfile\n",
    "import networkx as nx\n",
    "import shutil\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import community.community_louvain as community_louvain\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    result=[]\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        \n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n",
    "def get_topics_paper(text,\n",
    "                     num_topics = 10,\n",
    "                     num_words = 2):\n",
    "    \n",
    "    text = preprocess(text)\n",
    "    dictionary = gensim.corpora.Dictionary(np.array([text]))\n",
    "\n",
    "    # generate BOW\n",
    "    bow_corpus = [dictionary.doc2bow([doc]) for doc in text]\n",
    "\n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics = num_topics, \n",
    "                                       id2word = dictionary,                                    \n",
    "                                       passes = 10,\n",
    "                                       workers = 2)\n",
    "\n",
    "    lda_topics = lda_model.show_topics(num_words=num_words)\n",
    "\n",
    "    keywords = []\n",
    "    filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "    for topic in lda_topics:\n",
    "        for word in preprocess_string(topic[1], filters):        \n",
    "            keywords.append(word)\n",
    "        \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'get_topics_paper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-5fd25a09cb24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_PDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_PDF_tika\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m utils.get_topics_paper(text, \n\u001b[0m\u001b[1;32m     10\u001b[0m                  \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                  num_topics = 5)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'get_topics_paper'"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r'test_data'\n",
    "\n",
    "import utils\n",
    "\n",
    "fname = glob.glob(os.path.join(PDF_DIR, '*.pdf'))[0]\n",
    "\n",
    "text = open_PDF.open_PDF_tika(fname)\n",
    "\n",
    "utils.get_topics_paper(text, \n",
    "                 num_words = 5, \n",
    "                 num_topics = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the different topics per paper. We generate a dict where we store:\n",
    "\n",
    " - paper name\n",
    " - position\n",
    " - list of topics\n",
    " \n",
    "We then generate a networkx graph object. The papers that have similar topics as attributes are linked together and the edges are reinforced with each new common keyword.\n",
    "\n",
    "Once the network is generated, we run Louvain community building algorithm to sort the different papers based on their common topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_from_papers(PDF_DIR,\n",
    "                          num_words,\n",
    "                          num_topics):\n",
    "\n",
    "    paper_dictionnary = {}\n",
    "    num_words = 5\n",
    "    num_topics = 5\n",
    "\n",
    "    error_files = []\n",
    "\n",
    "    for fname in tqdm(glob.glob(os.path.join(PDF_DIR, '*.pdf'))):\n",
    "\n",
    "        try:\n",
    "\n",
    "            text = open_PDF.open_PDF_tika(fname)\n",
    "            keywords = get_topics_paper(text, \n",
    "                         num_words = num_words, \n",
    "                         num_topics = num_topics)\n",
    "\n",
    "            paper_name = os.path.basename(fname)\n",
    "            dir_name = os.path.dirname(fname)\n",
    "\n",
    "            paper_dictionnary[paper_name] = {}\n",
    "            paper_dictionnary[paper_name]['directory'] = dir_name\n",
    "            paper_dictionnary[paper_name]['full_path'] = fname\n",
    "            paper_dictionnary[paper_name]['keywords'] = keywords\n",
    "\n",
    "        except:\n",
    "\n",
    "            error_files.append(fname)\n",
    "           \n",
    "        if len(error_files)>0:\n",
    "            print('Error for: ')\n",
    "            print(error_files)\n",
    "            \n",
    "    return paper_dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle\n",
    "\n",
    "with open('paper_dictionnary.p', 'wb') as fp:\n",
    "    pickle.dump(paper_dictionnary, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph_from_dict(paper_dictionnary, \n",
    "                         no_keywords = ['http', 'ncbi', 'experi', 'pubm', 'elsevi', 'refhub', 'cell']):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(paper_dictionnary.keys())\n",
    "\n",
    "    for key_n in paper_dictionnary.keys():\n",
    "\n",
    "        for keyword in paper_dictionnary[key_n]['keywords']:\n",
    "\n",
    "            # explore other articles\n",
    "            for article in paper_dictionnary.keys():\n",
    "\n",
    "                # not current article\n",
    "                if article != key_n:\n",
    "\n",
    "                    # if keyword in both articles then link\n",
    "                    if (keyword in paper_dictionnary[article]['keywords']) & (keyword not in no_keywords):\n",
    "\n",
    "                        edge_info = G.get_edge_data(article, key_n)\n",
    "                        if edge_info is None:\n",
    "                            G.add_edge(article,key_n, weight = 1)\n",
    "                            G[article][key_n]['keywords'] = [keyword]\n",
    "                        elif (keyword not in edge_info['keywords']):\n",
    "                            w = edge_info['weight']\n",
    "                            G.add_edge(article,key_n, weight = w+1)\n",
    "                            G[article][key_n]['keywords'] += [keyword]\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = make_graph_from_dict(paper_dictionnary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(G)\n",
    "edgewidth = [G.get_edge_data(u, v)['weight'] for u, v in G.edges()]\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, \n",
    "                       alpha=0.3, \n",
    "                       width=edgewidth, \n",
    "                       edge_color=\"m\")\n",
    "nx.draw_networkx_nodes(G, pos, \n",
    "                       node_color=\"#210070\", \n",
    "                       alpha=0.9)\n",
    "\n",
    "#label_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\": 0.7}e\n",
    "#nx.draw_networkx_labels(G, pos,\n",
    "#                        font_size=9, \n",
    "#                        bbox=label_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=40, \n",
    "                        cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_all_keywords(paper_dictionnary):\n",
    "    \n",
    "    keyword_list = []\n",
    "    \n",
    "    for paper in paper_dictionnary.keys():\n",
    "        \n",
    "        keywords_paper = paper_dictionnary[paper]['keywords']\n",
    "        \n",
    "        keyword_list += keywords_paper\n",
    "        \n",
    "    return keyword_list\n",
    "\n",
    "def make_word_list(paper_dictionnary):\n",
    "    \n",
    "    keyword_list = get_all_keywords(paper_dictionnary)\n",
    "        \n",
    "    return Counter(keyword_list)\n",
    "\n",
    "def get_most_common_words(word_count, n):\n",
    "    \n",
    "    common_word_list = []\n",
    "    \n",
    "    most_common_words = word_count.most_common(n)\n",
    "    \n",
    "    for word_tuple in most_common_words:\n",
    "        \n",
    "        common_word_list.append(word_tuple[0])\n",
    "                \n",
    "    return common_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words by percentage of nodes that express this word! If more than $x\\%$ of the nodes express it then remove the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = make_word_list(paper_dictionnary)\n",
    "no_keywords = get_most_common_words(word_count, 20)\n",
    "no_keywords += ['http', 'ncbi', 'experi', 'pubm', 'elsevi', 'refhub', 'cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_cleaned = make_graph_from_dict(paper_dictionnary, no_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(G_cleaned)\n",
    "edgewidth = [G_cleaned.get_edge_data(u, v)['weight'] for u, v in G_cleaned.edges()]\n",
    "\n",
    "nx.draw_networkx_edges(G_cleaned, pos, \n",
    "                       alpha=0.3, \n",
    "                       width=edgewidth, \n",
    "                       edge_color=\"m\")\n",
    "nx.draw_networkx_nodes(G_cleaned, pos, \n",
    "                       node_color=\"#210070\", \n",
    "                       alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(G_cleaned, \n",
    "                                             resolution = 1.1)\n",
    "\n",
    "pos = nx.kamada_kawai_layout(G_cleaned)\n",
    "\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap('Set2', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(G_cleaned, \n",
    "                       pos, \n",
    "                       partition.keys(), \n",
    "                       node_size=40, \n",
    "                       cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G_cleaned, \n",
    "                       pos, \n",
    "                       alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(G_cleaned, \n",
    "                                             resolution = 1.1)\n",
    "\n",
    "for node in G_cleaned.nodes():\n",
    "    \n",
    "    G_cleaned.nodes[node]['partition'] = partition[node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the partition of the nodes. What we now want to do is:\n",
    " - associate a probability for each word to be in each community\n",
    " - find the relatively most probable words between the communities\n",
    " - associate each community with these most probable words\n",
    " \n",
    "Then within each community, build the subgraph with the most probable words removed and repeat the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_the_words(G):\n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    for u,v,d in G.edges(data = True):\n",
    "        \n",
    "        word_list += d['keywords']\n",
    "    \n",
    "    return Counter(word_list)\n",
    "\n",
    "def get_word_prob(counter_dict):\n",
    "        \n",
    "    total_word_number = np.sum([counter_dict[k] for k in counter_dict.keys()])\n",
    "    \n",
    "    counter_frame = pandas.DataFrame()\n",
    "    i = 0\n",
    "    \n",
    "    for word in counter_dict.keys():\n",
    "        \n",
    "        counter_frame.loc[word, 'prob'] = counter_dict[word]/total_word_number\n",
    "\n",
    "    return counter_frame\n",
    "\n",
    "def get_subgraph(G, attribute, attribute_value):\n",
    "\n",
    "    node_list = []\n",
    "\n",
    "    for x, y in G.nodes(data=True):\n",
    "\n",
    "        if (y[attribute] == attribute_value):\n",
    "\n",
    "            node_list.append(x)\n",
    "\n",
    "    return G.subgraph(node_list) \n",
    "    \n",
    "def prob_in_communities(G):\n",
    "    \n",
    "    partition = nx.get_node_attributes(G, 'partition')\n",
    "    partition_list = np.unique([partition[k] for k in partition.keys()])\n",
    "    \n",
    "    counter_dict = get_all_the_words(G)\n",
    "    tot_graph_counter_frame = get_word_prob(counter_dict)\n",
    "    \n",
    "    tot_graph_counter_frame.columns = ['whole_graph']\n",
    "    \n",
    "    for partition_value in partition_list:\n",
    "        \n",
    "        subgraph = get_subgraph(G, 'partition', partition_value)\n",
    "                \n",
    "        counter_dict = get_all_the_words(subgraph)\n",
    "        counter_frame = get_word_prob(counter_dict)\n",
    "        \n",
    "        if not counter_frame.empty:\n",
    "            counter_frame.columns = [str(int(partition_value))]\n",
    "        \n",
    "            tot_graph_counter_frame = tot_graph_counter_frame.merge(counter_frame,\n",
    "                                                                left_index=True,\n",
    "                                                                right_index=True,\n",
    "                                                                how = 'outer')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            counter_frame = pandas.DataFrame(data = np.zeros(len(tot_graph_counter_frame)))\n",
    "            counter_frame.columns = [str(int(partition_value))]\n",
    "            counter_frame.index = tot_graph_counter_frame.index.values\n",
    "            \n",
    "            tot_graph_counter_frame = tot_graph_counter_frame.merge(counter_frame,\n",
    "                                                                left_index=True,\n",
    "                                                                right_index=True,\n",
    "                                                                how = 'outer')\n",
    "            \n",
    "    return tot_graph_counter_frame.fillna(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(word_prob, column):\n",
    "    \n",
    "    if not column in word_prob.columns:\n",
    "        print(column)\n",
    "        print(word_prob)\n",
    "\n",
    "    entropy_frame = pandas.DataFrame()\n",
    "\n",
    "    for j in word_prob.columns:\n",
    "        \n",
    "        if column != j:\n",
    "\n",
    "            s = -np.log(word_prob[j]/word_prob[column])\n",
    "            s = pandas.DataFrame(s)\n",
    "            s.columns = [j]\n",
    "            entropy_frame = entropy_frame.merge(s,\n",
    "                                                  left_index=True,\n",
    "                                                  right_index=True,\n",
    "                                                  how = 'outer')\n",
    "        \n",
    "    return pandas.DataFrame(entropy_frame.sum(axis = 1))\n",
    "\n",
    "def sort_papers_from_graph(G,\n",
    "                           PDF_DIR,\n",
    "                           SORTED_DIR,\n",
    "                           n_largest_names,\n",
    "                           n_largest_description):\n",
    "    \n",
    "    partition = community_louvain.best_partition(G, \n",
    "                                                 resolution = partition_resolution)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['partition'] = partition[node]\n",
    "    \n",
    "    word_prob = prob_in_communities(G)\n",
    "    partition = nx.get_node_attributes(G, 'partition')\n",
    "    partition_list = np.unique([partition[k] for k in partition.keys()])\n",
    "\n",
    "    # pour éviter les betises\n",
    "    word_prob += 1e-10\n",
    "    \n",
    "    if os.path.exists(SORTED_DIR):\n",
    "        shutil.rmtree(SORTED_DIR) \n",
    "    \n",
    "    if not os.path.exists(SORTED_DIR):\n",
    "        os.makedirs(SORTED_DIR)\n",
    "    \n",
    "    \n",
    "    for partition_number in partition_list:\n",
    "        \n",
    "        # need to string the name\n",
    "        partition_name = str(int(partition_number))\n",
    "        \n",
    "        s = get_unique_words(word_prob, partition_name)\n",
    "        name_frame = s.nlargest(n_largest_names, columns = 0)\n",
    "                \n",
    "        # make the folder\n",
    "        folder_name = ''\n",
    "        for ind in name_frame.index:\n",
    "            folder_name += ind +'_' \n",
    "        \n",
    "        if not os.path.exists(os.path.join(SORTED_DIR, folder_name)):\n",
    "            os.mkdir(os.path.join(SORTED_DIR, folder_name))\n",
    "            \n",
    "        # text file description of the folder\n",
    "        word_frame = s.nlargest(n_largest_description, columns = 0)\n",
    "        word_list = [i for i in word_frame.index]\n",
    "        write_description(os.path.join(SORTED_DIR, folder_name),\n",
    "                          word_list)\n",
    "        \n",
    "        for article_name in partition.keys():\n",
    "            \n",
    "            src = os.path.join(PDF_DIR, article_name)\n",
    "            dst = os.path.join(SORTED_DIR, folder_name,article_name)\n",
    "            \n",
    "            if partition[article_name] == partition_number:\n",
    "            \n",
    "                copyfile(src, dst)\n",
    "            \n",
    "    return\n",
    "\n",
    "def write_description(DIR, word_list):\n",
    "    \n",
    "    with open(os.path.join(DIR, 'readme.txt'), 'w') as f:\n",
    "        for line in word_list:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    return\n",
    "        \n",
    "def savedict(dict_to_save,\n",
    "             DIR):\n",
    "    \n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except ImportError:  # Python 3.x\n",
    "        import pickle\n",
    "        \n",
    "    if os.path.exists(DIR):\n",
    "        shutil.rmtree(DIR) \n",
    "    \n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    with open(os.path.join(DIR,'paper_dictionnary.p'), 'wb') as fp:\n",
    "        pickle.dump(dict_to_save, \n",
    "                    fp, \n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def sort_papers_based_on_contents(PDF_DIR,\n",
    "                                  SORTED_DIR,\n",
    "                                  max_common_words,\n",
    "                                  num_words,\n",
    "                                  num_topics,\n",
    "                                  n_largest_names,\n",
    "                                  partition_resolution,\n",
    "                                  n_largest_description,\n",
    "                                  SAVEDICT:bool,\n",
    "                                  DICTDIR,\n",
    "                                  no_keywords = ['http', 'ncbi', 'experi', 'biorxiv', 'pubm', 'elsevi', 'refhub']):\n",
    "    \n",
    "    paper_dictionnary = make_dict_from_papers(PDF_DIR,\n",
    "                          num_words,\n",
    "                          num_topics)\n",
    "    \n",
    "    if SAVEDICT:\n",
    "         savedict(paper_dictionnary,\n",
    "                  DICTDIR)\n",
    "            \n",
    "    \n",
    "    word_count = make_word_list(paper_dictionnary)\n",
    "    no_keywords += get_most_common_words(word_count, max_common_words)\n",
    "    \n",
    "    G = make_graph_from_dict(paper_dictionnary, no_keywords)\n",
    "        \n",
    "    sort_papers_from_graph(G, \n",
    "                           PDF_DIR,\n",
    "                           SORTED_DIR,\n",
    "                           n_largest_names,\n",
    "                           n_largest_description)\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_papers_from_dict(DICTNAME,\n",
    "                          PDF_DIR,\n",
    "                          SORTED_DIR,\n",
    "                          max_common_words,\n",
    "                          n_largest_names,\n",
    "                          partition_resolution,\n",
    "                          n_largest_description,\n",
    "                          no_keywords = ['http', 'ncbi', 'experi', 'biorxiv', 'pubm', 'elsevi', 'refhub']):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except ImportError:  # Python 3.x\n",
    "        import pickle\n",
    "\n",
    "    with open(DICTNAME, 'rb') as fp:\n",
    "        paper_dictionnary = pickle.load(fp)\n",
    "    \n",
    "    word_count = make_word_list(paper_dictionnary)\n",
    "    no_keywords += get_most_common_words(word_count, max_common_words)\n",
    "    \n",
    "    G = make_graph_from_dict(paper_dictionnary, no_keywords)\n",
    "\n",
    "    partition = community_louvain.best_partition(G, \n",
    "                                                 resolution = partition_resolution)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['partition'] = partition[node]\n",
    "        \n",
    "    sort_papers_from_graph(G, \n",
    "                           PDF_DIR,\n",
    "                           SORTED_DIR,\n",
    "                           n_largest_names,\n",
    "                           n_largest_description)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test from dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SORTED_DIR = r'sorted_test_data_dict'\n",
    "PDF_DIR = r'test_data'\n",
    "DICTNAME = 'paper_dictionnary.p'\n",
    "n_largest_names = 2\n",
    "partition_resolution = 0.9\n",
    "n_largest_description = 9\n",
    "max_common_words = 2\n",
    "\n",
    "no_keywords = ['http', \n",
    "               'ncbi', \n",
    "               'experi', \n",
    "               'biorxiv', \n",
    "               'pubm', \n",
    "               'elsevi', \n",
    "               'refhub',\n",
    "               'dataset',\n",
    "               'licens',\n",
    "               'grant',\n",
    "               'holder',\n",
    "               'preprint',\n",
    "               'copyright',\n",
    "               'dataset',\n",
    "               'funder',\n",
    "               'intern',\n",
    "               'ncem',\n",
    "               'requir',\n",
    "               'creativecommon',\n",
    "               'certifi']\n",
    "\n",
    "sort_papers_from_dict(DICTNAME,\n",
    "                      PDF_DIR,\n",
    "                      SORTED_DIR,\n",
    "                      max_common_words,\n",
    "                      n_largest_names,\n",
    "                      partition_resolution,\n",
    "                      n_largest_description,\n",
    "                      no_keywords = no_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b23a317aeae4191a1a24eac1f158137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PDF_DIR = r'test_data_bis'\n",
    "SORTED_DIR = r'sorted_test_data'\n",
    "\n",
    "n_largest_names = 2\n",
    "partition_resolution = 0.9\n",
    "n_largest_description = 9\n",
    "\n",
    "sort_papers_based_on_contents(PDF_DIR,\n",
    "                              SORTED_DIR,\n",
    "                              max_common_words = 2,\n",
    "                              num_words = 4,\n",
    "                              num_topics = 4,\n",
    "                              n_largest_names = 2,\n",
    "                              partition_resolution = 0.9,\n",
    "                              n_largest_description = 10,\n",
    "                              SAVEDICT = True,\n",
    "                              DICTDIR = PDF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_sorter_from_graph(G,\n",
    "                           PDF_DIR,\n",
    "                           DESTINATION_DIR,\n",
    "                           n_largest_names,\n",
    "                           n_largest_description,\n",
    "                           min_graph_size,\n",
    "                           partition_resolution,\n",
    "                           word_list,\n",
    "                           iteration,\n",
    "                           max_depth):\n",
    "    \n",
    "    # if the graph is too small to partition stop here\n",
    "    if (len(G) < min_graph_size) | (iteration+1 > max_depth):\n",
    "        \n",
    "        move_articles(G,\n",
    "                  PDF_DIR,\n",
    "                  DESTINATION_DIR)\n",
    "        \n",
    "        # text file description of the folder\n",
    "        write_description(DESTINATION_DIR,\n",
    "                          word_list)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['partition'] = np.nan\n",
    "         \n",
    "    # partition the graph\n",
    "    partition = community_louvain.best_partition(G, \n",
    "                                                 resolution = partition_resolution)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['partition'] = partition[node]\n",
    "    \n",
    "    word_prob = prob_in_communities(G)\n",
    "    partition = nx.get_node_attributes(G, 'partition')\n",
    "    partition_list = np.unique([partition[k] for k in partition.keys()])\n",
    "\n",
    "    # pour éviter les betises\n",
    "    word_prob += 1e-10\n",
    "    \n",
    "    # prepare the folders\n",
    "    if os.path.exists(DESTINATION_DIR):\n",
    "        shutil.rmtree(DESTINATION_DIR) \n",
    "    \n",
    "    if not os.path.exists(DESTINATION_DIR):\n",
    "        os.makedirs(DESTINATION_DIR)\n",
    "    \n",
    "    # run the partition proper\n",
    "    for partition_number in partition_list:\n",
    "        \n",
    "        # need to string the name\n",
    "        partition_name = str(int(partition_number))\n",
    "        s = get_unique_words(word_prob, partition_name)\n",
    "        name_frame = s.nlargest(n_largest_names, columns = 0)\n",
    "                \n",
    "        # make the folder\n",
    "        folder_name = ''\n",
    "        for ind in name_frame.index:\n",
    "            folder_name += ind +'_'\n",
    "            \n",
    "        word_frame = s.nlargest(n_largest_description, columns = 0)\n",
    "        word_list = [i for i in word_frame.index]\n",
    "        \n",
    "        if not os.path.exists(os.path.join(DESTINATION_DIR, folder_name)):\n",
    "            os.mkdir(os.path.join(DESTINATION_DIR, folder_name))\n",
    "            \n",
    "        LOCAL_DESTINATION_DIR = os.path.join(DESTINATION_DIR, folder_name)\n",
    "        \n",
    "        # make local graph\n",
    "        subgraph = get_subgraph(G, 'partition', partition_number)\n",
    "    \n",
    "        recursive_sorter_from_graph(G = subgraph,\n",
    "                           PDF_DIR = PDF_DIR,\n",
    "                           DESTINATION_DIR = LOCAL_DESTINATION_DIR,\n",
    "                           n_largest_names = n_largest_names,\n",
    "                           n_largest_description = n_largest_description,\n",
    "                           min_graph_size = min_graph_size,\n",
    "                           partition_resolution = partition_resolution,\n",
    "                           word_list = word_list,\n",
    "                           iteration = iteration + 1,\n",
    "                           max_depth = max_depth)\n",
    "            \n",
    "    return\n",
    "\n",
    "def move_articles(G,\n",
    "                  PDF_DIR,\n",
    "                  DESTINATION_DIR):\n",
    "    \n",
    "    for article_name in G.nodes():\n",
    "\n",
    "        src = os.path.join(PDF_DIR, article_name)\n",
    "        dst = os.path.join(DESTINATION_DIR, article_name)\n",
    "\n",
    "        copyfile(src, dst)\n",
    "        \n",
    "    return\n",
    "\n",
    "def recursive_sort_from_dict(DICTNAME,\n",
    "                          PDF_DIR,\n",
    "                          SORTED_DIR,\n",
    "                          max_common_words,\n",
    "                          n_largest_names,\n",
    "                          partition_resolution,\n",
    "                          n_largest_description,\n",
    "                          min_graph_size,\n",
    "                          no_keywords = ['http', 'ncbi', 'experi', 'biorxiv', 'pubm', 'elsevi', 'refhub'],\n",
    "                          iteration = 0,\n",
    "                          max_depth = 5):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        import cPickle as pickle\n",
    "    except ImportError:  # Python 3.x\n",
    "        import pickle\n",
    "\n",
    "    with open(DICTNAME, 'rb') as fp:\n",
    "        paper_dictionnary = pickle.load(fp)\n",
    "    \n",
    "    word_count = make_word_list(paper_dictionnary)\n",
    "    no_keywords += get_most_common_words(word_count, max_common_words)\n",
    "    \n",
    "    G = make_graph_from_dict(paper_dictionnary, no_keywords)\n",
    "\n",
    "    partition = community_louvain.best_partition(G, \n",
    "                                                 resolution = partition_resolution)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['partition'] = partition[node]\n",
    "        \n",
    "    recursive_sorter_from_graph(G,\n",
    "                           PDF_DIR = PDF_DIR,\n",
    "                           DESTINATION_DIR = SORTED_DIR,\n",
    "                           n_largest_names = n_largest_names,\n",
    "                           n_largest_description = n_largest_description,\n",
    "                           min_graph_size = min_graph_size,\n",
    "                           partition_resolution = partition_resolution,\n",
    "                           word_list = [],\n",
    "                           iteration = iteration,\n",
    "                           max_depth = max_depth)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "SORTED_DIR = r'recusrive_sort_test_data_dict'\n",
    "PDF_DIR = r'test_data'\n",
    "DICTNAME = 'paper_dictionnary.p'\n",
    "n_largest_names = 2\n",
    "partition_resolution = 0.85\n",
    "n_largest_description = 9\n",
    "max_common_words = 2\n",
    "min_graph_size = 25\n",
    "\n",
    "iteration = 0\n",
    "max_depth = 4\n",
    "\n",
    "no_keywords = ['http', \n",
    "               'ncbi', \n",
    "               'experi', \n",
    "               'biorxiv', \n",
    "               'pubm', \n",
    "               'elsevi', \n",
    "               'refhub',\n",
    "               'dataset',\n",
    "               'licens',\n",
    "               'grant',\n",
    "               'holder',\n",
    "               'preprint',\n",
    "               'copyright',\n",
    "               'dataset',\n",
    "               'funder',\n",
    "               'intern',\n",
    "               'ncem',\n",
    "               'requir',\n",
    "               'creativecommon',\n",
    "               'certifi',\n",
    "               'version',\n",
    "               'fund',\n",
    "               'research']\n",
    "\n",
    "recursive_sort_from_dict(DICTNAME,\n",
    "                          PDF_DIR,\n",
    "                          SORTED_DIR,\n",
    "                          max_common_words,\n",
    "                          n_largest_names,\n",
    "                          partition_resolution,\n",
    "                          n_largest_description,\n",
    "                          min_graph_size,\n",
    "                          no_keywords = no_keywords,\n",
    "                          iteration = iteration,\n",
    "                          max_depth = max_depth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
